{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import random\n",
    "import config\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# NEURAL NETWORK MODULES/LAYERS\n",
    "from dataset_224 import Dataset\n",
    "# METRICS CLASS FOR EVALUATION\n",
    "from metrics import Metrics\n",
    "# CONFIG PARSER\n",
    "from config import parse_args\n",
    "# TRAIN AND TEST HELPER FUNCTIONS\n",
    "from trainer import Trainer\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import model_vgg16\n",
    "# import model_vgg19\n",
    "# import model_resnet50\n",
    "# import model_densenet121\n",
    "# import model_inceptionv3\n",
    "\n",
    "# MAIN BLOCK\n",
    "# def main():\n",
    "# global args\n",
    "# args = parse_args()\n",
    "# global logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.data='./data'\n",
    "        self.save='checkpoints/'\n",
    "        self.expname='vgg16_ensemble_224'\n",
    "        self.expno=0\n",
    "        self.pretrained_model='vgg16'\n",
    "        self.num_classes=16\n",
    "        # training arguments\n",
    "        self.epochs=100000\n",
    "        self.batchsize=4\n",
    "        self.lr=0.0001\n",
    "        self.wd=1e-4\n",
    "        self.sparse=False\n",
    "        self.optim='adam'\n",
    "        # miscellaneous options\n",
    "        self.seed=123\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.cuda else \"cpu\")\n",
    "        self.pretrained_holistic=2\n",
    "\n",
    "#         cuda_parser = parser.add_mutually_exclusive_group(required=False)\n",
    "#         cuda', dest='cuda', action='store_true')\n",
    "#         cuda_parser.add_argument('--no-cuda', dest='cuda', action='store_false')\n",
    "#         parser.set_defaults(cuda=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = config()\n",
    "writer = SummaryWriter(comment = args.expname)\n",
    "args.batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(args, parameters):\n",
    "    if args.optim=='adam':\n",
    "        optimizer   = optim.Adam(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "    elif args.optim=='adagrad':\n",
    "        optimizer   = optim.Adagrad(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "    elif args.optim=='sgd':\n",
    "        optimizer   = optim.SGD(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "    elif args.optim == 'adadelta':\n",
    "        optimizer = optim.Adadelta(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model exists\n"
     ]
    }
   ],
   "source": [
    "# initialize model, criterion/loss_function, optimizer\n",
    "pretrained_vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze training for all layers\n",
    "# for child in pretrained_vgg16.children():\n",
    "#     for param in child.parameters():\n",
    "#         param.requires_grad = False\n",
    "start_point = 0\n",
    "if args.pretrained_holistic == 0 and 'multiple_loss' not in args.expname:\n",
    "    \n",
    "    model = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    optimizer = create_optimizer(args, list(model.parameters()))\n",
    "    print('here - no multiple loss')\n",
    "    if os.path.exists('./checkpoints/{0}.pt'.format(args.expname)):\n",
    "        print('here again')\n",
    "        checkpoint = torch.load('./checkpoints/{0}.pt'.format(args.expname))\n",
    "        print(checkpoint.keys())\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "        else:\n",
    "            start_point = 0\n",
    "#         optimizer.load_state_dict(checkpoint['optim'])\n",
    "\n",
    "elif args.pretrained_holistic == 0 and 'multiple_loss' in args.expname:\n",
    "    print('here - multiple loss')\n",
    "    model = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    optimizer = create_optimizer(args, list(model.parameters()))\n",
    "    if os.path.exists('./checkpoints/{0}.pt'.format(args.expname)):\n",
    "        checkpoint = torch.load('./checkpoints/{0}.pt'.format(args.expname))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "elif args.pretrained_holistic == 1:\n",
    "    checkpoint_holistic = torch.load('./checkpoints/vgg_holistic.pt')\n",
    "    vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_holistic_model = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_holistic_model.load_state_dict(checkpoint_holistic['model'])\n",
    "\n",
    "    model = model_vgg16.DocClassificationRest(args, vgg16, pretrained_holistic_model)\n",
    "\n",
    "    parameters = list(pretrained_holistic_model.parameters()) #+ list(model.parameters()) + list(vgg16.parameters())\n",
    "\n",
    "    if os.path.exists('./checkpoints/{0}.pt'.format(args.expname)):\n",
    "        checkpoint_ensemble = torch.load('./checkpoints/{0}.pt'.format(args.expname))\n",
    "        model.load_state_dict(checkpoint_ensemble['model'])\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        start_point = checkpoint_ensemble['epoch']\n",
    "        optimizer = create_optimizer(args, parameters)\n",
    "        optimizer.load_state_dict(checkpoint_ensemble['optim'])\n",
    "    else:\n",
    "        optimizer = create_optimizer(args, parameters)\n",
    "        #optimizer.load_state_dict(checkpoint_holistic['optim'])  \n",
    "        \n",
    "elif args.pretrained_holistic == 2:\n",
    "    checkpoint_holistic = torch.load('./checkpoints/vgg16_holistic.pt')\n",
    "#     vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_holistic_model = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_holistic_model.load_state_dict(checkpoint_holistic['model'])\n",
    "    for child in pretrained_holistic_model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    checkpoint_header = torch.load('./checkpoints/vgg16_header.pt')\n",
    "#     vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_header_model = model_vgg16.DocClassificationRest(args, pretrained_vgg16, pretrained_holistic_model)\n",
    "    pretrained_header_model.load_state_dict(checkpoint_header['model'])\n",
    "    for child in pretrained_header_model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    checkpoint_footer = torch.load('./checkpoints/vgg16_footer.pt')\n",
    "#     vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_footer_model = model_vgg16.DocClassificationRest(args, pretrained_vgg16, pretrained_holistic_model)\n",
    "    pretrained_footer_model.load_state_dict(checkpoint_footer['model'])\n",
    "    \n",
    "    for child in pretrained_footer_model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    checkpoint_left_body = torch.load('./checkpoints/vgg16_left_body.pt')\n",
    "#     vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_left_body_model = model_vgg16.DocClassificationRest(args, pretrained_vgg16, pretrained_holistic_model)\n",
    "    pretrained_left_body_model.load_state_dict(checkpoint_left_body['model'])\n",
    "    \n",
    "    for child in pretrained_left_body_model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    checkpoint_right_body = torch.load('./checkpoints/vgg16_right_body.pt')\n",
    "#     vgg16 = model_vgg16.DocClassificationHolistic(args, pretrained_vgg16)\n",
    "    pretrained_right_body_model = model_vgg16.DocClassificationRest(args, pretrained_vgg16, pretrained_holistic_model)\n",
    "    pretrained_right_body_model.load_state_dict(checkpoint_right_body['model'])\n",
    "\n",
    "    \n",
    "    \n",
    "    for child in pretrained_right_body_model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    model = model_vgg16.DocClassificationEnsemble(args\n",
    "                                              , pretrained_holistic_model\n",
    "                                              , pretrained_header_model\n",
    "                                              , pretrained_footer_model\n",
    "                                              , pretrained_left_body_model\n",
    "                                              , pretrained_right_body_model)\n",
    "    \n",
    "    parameters = list(model.parameters()) #+ list(model.parameters()) + list(vgg16.parameters())\n",
    "\n",
    "    if os.path.exists('./checkpoints/{0}.pt'.format(args.expname)):\n",
    "        print('model exists')\n",
    "        checkpoint_ensemble = torch.load('./checkpoints/{0}.pt'.format(args.expname))\n",
    "        model.load_state_dict(checkpoint_ensemble['model'])\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        start_point = checkpoint_ensemble['epoch']\n",
    "        optimizer = create_optimizer(args, parameters)\n",
    "        optimizer.load_state_dict(checkpoint_ensemble['optim'])\n",
    "    else:\n",
    "        print('new model')\n",
    "        optimizer = create_optimizer(args, parameters)\n",
    "        #optimizer.load_state_dict(checkpoint_holistic['optim'])  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda(), criterion.cuda()\n",
    "\n",
    "\n",
    "metrics = Metrics(args.num_classes)\n",
    "\n",
    "# create trainer object for training and testing\n",
    "trainer = Trainer(args, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocClassificationEnsemble(\n",
      "  (pretrained_holistic): DocClassificationHolistic(\n",
      "    (pretrained_model): VGG(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (8): ReLU(inplace=True)\n",
      "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (11): ReLU(inplace=True)\n",
      "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (15): ReLU(inplace=True)\n",
      "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (18): ReLU(inplace=True)\n",
      "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (20): ReLU(inplace=True)\n",
      "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (22): ReLU(inplace=True)\n",
      "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (25): ReLU(inplace=True)\n",
      "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (27): ReLU(inplace=True)\n",
      "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (29): ReLU(inplace=True)\n",
      "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "      (classifier): Sequential(\n",
      "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Dropout(p=0.5, inplace=False)\n",
      "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): Dropout(p=0.5, inplace=False)\n",
      "        (6): Linear(in_features=4096, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pretrained_header): DocClassificationRest(\n",
      "    (pretrained_holistic): DocClassificationHolistic(\n",
      "      (pretrained_model): VGG(\n",
      "        (features): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): ReLU(inplace=True)\n",
      "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (8): ReLU(inplace=True)\n",
      "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (11): ReLU(inplace=True)\n",
      "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (13): ReLU(inplace=True)\n",
      "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (15): ReLU(inplace=True)\n",
      "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (18): ReLU(inplace=True)\n",
      "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (20): ReLU(inplace=True)\n",
      "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (22): ReLU(inplace=True)\n",
      "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (25): ReLU(inplace=True)\n",
      "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (27): ReLU(inplace=True)\n",
      "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (29): ReLU(inplace=True)\n",
      "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (classifier): Sequential(\n",
      "          (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Dropout(p=0.5, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "          (5): Dropout(p=0.5, inplace=False)\n",
      "          (6): Linear(in_features=4096, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pretrained_footer): DocClassificationRest(\n",
      "    (pretrained_holistic): DocClassificationHolistic(\n",
      "      (pretrained_model): VGG(\n",
      "        (features): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): ReLU(inplace=True)\n",
      "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (8): ReLU(inplace=True)\n",
      "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (11): ReLU(inplace=True)\n",
      "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (13): ReLU(inplace=True)\n",
      "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (15): ReLU(inplace=True)\n",
      "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (18): ReLU(inplace=True)\n",
      "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (20): ReLU(inplace=True)\n",
      "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (22): ReLU(inplace=True)\n",
      "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (25): ReLU(inplace=True)\n",
      "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (27): ReLU(inplace=True)\n",
      "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (29): ReLU(inplace=True)\n",
      "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (classifier): Sequential(\n",
      "          (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Dropout(p=0.5, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "          (5): Dropout(p=0.5, inplace=False)\n",
      "          (6): Linear(in_features=4096, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pretrained_left_body): DocClassificationRest(\n",
      "    (pretrained_holistic): DocClassificationHolistic(\n",
      "      (pretrained_model): VGG(\n",
      "        (features): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): ReLU(inplace=True)\n",
      "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (8): ReLU(inplace=True)\n",
      "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (11): ReLU(inplace=True)\n",
      "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (13): ReLU(inplace=True)\n",
      "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (15): ReLU(inplace=True)\n",
      "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (18): ReLU(inplace=True)\n",
      "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (20): ReLU(inplace=True)\n",
      "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (22): ReLU(inplace=True)\n",
      "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (25): ReLU(inplace=True)\n",
      "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (27): ReLU(inplace=True)\n",
      "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (29): ReLU(inplace=True)\n",
      "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (classifier): Sequential(\n",
      "          (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Dropout(p=0.5, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "          (5): Dropout(p=0.5, inplace=False)\n",
      "          (6): Linear(in_features=4096, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pretrained_right_body): DocClassificationRest(\n",
      "    (pretrained_holistic): DocClassificationHolistic(\n",
      "      (pretrained_model): VGG(\n",
      "        (features): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): ReLU(inplace=True)\n",
      "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (8): ReLU(inplace=True)\n",
      "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (11): ReLU(inplace=True)\n",
      "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (13): ReLU(inplace=True)\n",
      "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (15): ReLU(inplace=True)\n",
      "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (18): ReLU(inplace=True)\n",
      "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (20): ReLU(inplace=True)\n",
      "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (22): ReLU(inplace=True)\n",
      "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (25): ReLU(inplace=True)\n",
      "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (27): ReLU(inplace=True)\n",
      "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (29): ReLU(inplace=True)\n",
      "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "        (classifier): Sequential(\n",
      "          (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Dropout(p=0.5, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (4): ReLU(inplace=True)\n",
      "          (5): Dropout(p=0.5, inplace=False)\n",
      "          (6): Linear(in_features=4096, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.75, inplace=False)\n",
      "  (ff1): Linear(in_features=80, out_features=256, bias=True)\n",
      "  (ff2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=16, bias=True)\n",
      ")\n",
      "Param Name: pretrained_holistic.pretrained_model.features.0.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.0.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.2.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.2.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.5.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.5.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.7.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.7.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.10.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.10.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.12.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.12.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.14.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.14.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.17.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.17.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.19.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.19.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.21.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.21.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.24.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.24.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.26.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.26.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.28.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.features.28.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.0.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.0.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.3.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.3.bias Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.6.weight Requires Training: False\n",
      "Param Name: pretrained_holistic.pretrained_model.classifier.6.bias Requires Training: False\n",
      "Param Name: ff1.weight Requires Training: True\n",
      "Param Name: ff1.bias Requires Training: True\n",
      "Param Name: ff2.weight Requires Training: True\n",
      "Param Name: ff2.bias Requires Training: True\n",
      "Param Name: output.weight Requires Training: True\n",
      "Param Name: output.bias Requires Training: True\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for parameter, name in zip(model.parameters(), model.named_parameters()):\n",
    "     print(\"Param Name: \" + name[0] + \" Requires Training: \" + str(parameter.requires_grad))\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Size of test data   : 15999 \n"
     ]
    }
   ],
   "source": [
    "# train_dir = glob.glob(os.path.join(args.data,'train/holistic/*.jpg'))\n",
    "# dev_dir = glob.glob(os.path.join(args.data,'val/holistic/*.jpg'))\n",
    "# test_dir = glob.glob(os.path.join(args.data,'test/holistic/*.jpg'))\n",
    "\n",
    "# print(len(train_dir), len(dev_dir), len(test_dir))\n",
    "\n",
    "# 'data/labels/train.csv'\n",
    "# train_dataset = Dataset('data/stratified/rvl-cdip/labels/train.csv',model_type=args.pretrained_model)\n",
    "# dev_dataset = Dataset('data/stratified/rvl-cdip/labels/val.csv', model_type=args.pretrained_model)\n",
    "test_dataset = Dataset('data/stratified/rvl-cdip/labels/test.csv', model_type=args.pretrained_model)\n",
    "\n",
    "# print('==> Size of train data   : %d ' % len(train_dataset))\n",
    "# print('==> Size of val data   : %d ' % len(dev_dataset))\n",
    "print('==> Size of test data   : %d ' % len(test_dataset))\n",
    "\n",
    "# train_idx = list(np.arange(len(train_dataset)))\n",
    "# dev_idx = list(np.arange(len(dev_dataset)))\n",
    "test_idx = list(np.arange(len(test_dataset)))\n",
    "\n",
    "best = float('inf')\n",
    "columns = ['ExpName','ExpNo', 'Epoch', 'Loss','Accuracy']\n",
    "results = []\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "            Conv2d-2         [-1, 64, 224, 224]           1,792\n",
      "            Conv2d-3         [-1, 64, 224, 224]           1,792\n",
      "            Conv2d-4         [-1, 64, 224, 224]           1,792\n",
      "            Conv2d-5         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-6         [-1, 64, 224, 224]               0\n",
      "              ReLU-7         [-1, 64, 224, 224]               0\n",
      "              ReLU-8         [-1, 64, 224, 224]               0\n",
      "              ReLU-9         [-1, 64, 224, 224]               0\n",
      "             ReLU-10         [-1, 64, 224, 224]               0\n",
      "           Conv2d-11         [-1, 64, 224, 224]          36,928\n",
      "           Conv2d-12         [-1, 64, 224, 224]          36,928\n",
      "           Conv2d-13         [-1, 64, 224, 224]          36,928\n",
      "           Conv2d-14         [-1, 64, 224, 224]          36,928\n",
      "           Conv2d-15         [-1, 64, 224, 224]          36,928\n",
      "             ReLU-16         [-1, 64, 224, 224]               0\n",
      "             ReLU-17         [-1, 64, 224, 224]               0\n",
      "             ReLU-18         [-1, 64, 224, 224]               0\n",
      "             ReLU-19         [-1, 64, 224, 224]               0\n",
      "             ReLU-20         [-1, 64, 224, 224]               0\n",
      "        MaxPool2d-21         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-22         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-23         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-24         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-25         [-1, 64, 112, 112]               0\n",
      "           Conv2d-26        [-1, 128, 112, 112]          73,856\n",
      "           Conv2d-27        [-1, 128, 112, 112]          73,856\n",
      "           Conv2d-28        [-1, 128, 112, 112]          73,856\n",
      "           Conv2d-29        [-1, 128, 112, 112]          73,856\n",
      "           Conv2d-30        [-1, 128, 112, 112]          73,856\n",
      "             ReLU-31        [-1, 128, 112, 112]               0\n",
      "             ReLU-32        [-1, 128, 112, 112]               0\n",
      "             ReLU-33        [-1, 128, 112, 112]               0\n",
      "             ReLU-34        [-1, 128, 112, 112]               0\n",
      "             ReLU-35        [-1, 128, 112, 112]               0\n",
      "           Conv2d-36        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-37        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-38        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-39        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-40        [-1, 128, 112, 112]         147,584\n",
      "             ReLU-41        [-1, 128, 112, 112]               0\n",
      "             ReLU-42        [-1, 128, 112, 112]               0\n",
      "             ReLU-43        [-1, 128, 112, 112]               0\n",
      "             ReLU-44        [-1, 128, 112, 112]               0\n",
      "             ReLU-45        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-46          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-47          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-48          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-49          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-50          [-1, 128, 56, 56]               0\n",
      "           Conv2d-51          [-1, 256, 56, 56]         295,168\n",
      "           Conv2d-52          [-1, 256, 56, 56]         295,168\n",
      "           Conv2d-53          [-1, 256, 56, 56]         295,168\n",
      "           Conv2d-54          [-1, 256, 56, 56]         295,168\n",
      "           Conv2d-55          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-56          [-1, 256, 56, 56]               0\n",
      "             ReLU-57          [-1, 256, 56, 56]               0\n",
      "             ReLU-58          [-1, 256, 56, 56]               0\n",
      "             ReLU-59          [-1, 256, 56, 56]               0\n",
      "             ReLU-60          [-1, 256, 56, 56]               0\n",
      "           Conv2d-61          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-62          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-63          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-64          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-65          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-66          [-1, 256, 56, 56]               0\n",
      "             ReLU-67          [-1, 256, 56, 56]               0\n",
      "             ReLU-68          [-1, 256, 56, 56]               0\n",
      "             ReLU-69          [-1, 256, 56, 56]               0\n",
      "             ReLU-70          [-1, 256, 56, 56]               0\n",
      "           Conv2d-71          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-72          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-73          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-74          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-75          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-76          [-1, 256, 56, 56]               0\n",
      "             ReLU-77          [-1, 256, 56, 56]               0\n",
      "             ReLU-78          [-1, 256, 56, 56]               0\n",
      "             ReLU-79          [-1, 256, 56, 56]               0\n",
      "             ReLU-80          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-81          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-82          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-83          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-84          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-85          [-1, 256, 28, 28]               0\n",
      "           Conv2d-86          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-87          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-88          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-89          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-90          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-91          [-1, 512, 28, 28]               0\n",
      "             ReLU-92          [-1, 512, 28, 28]               0\n",
      "             ReLU-93          [-1, 512, 28, 28]               0\n",
      "             ReLU-94          [-1, 512, 28, 28]               0\n",
      "             ReLU-95          [-1, 512, 28, 28]               0\n",
      "           Conv2d-96          [-1, 512, 28, 28]       2,359,808\n",
      "           Conv2d-97          [-1, 512, 28, 28]       2,359,808\n",
      "           Conv2d-98          [-1, 512, 28, 28]       2,359,808\n",
      "           Conv2d-99          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-100          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-101          [-1, 512, 28, 28]               0\n",
      "            ReLU-102          [-1, 512, 28, 28]               0\n",
      "            ReLU-103          [-1, 512, 28, 28]               0\n",
      "            ReLU-104          [-1, 512, 28, 28]               0\n",
      "            ReLU-105          [-1, 512, 28, 28]               0\n",
      "          Conv2d-106          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-107          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-108          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-109          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-110          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-111          [-1, 512, 28, 28]               0\n",
      "            ReLU-112          [-1, 512, 28, 28]               0\n",
      "            ReLU-113          [-1, 512, 28, 28]               0\n",
      "            ReLU-114          [-1, 512, 28, 28]               0\n",
      "            ReLU-115          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-116          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-117          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-118          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-119          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-120          [-1, 512, 14, 14]               0\n",
      "          Conv2d-121          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-122          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-123          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-124          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-125          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-126          [-1, 512, 14, 14]               0\n",
      "            ReLU-127          [-1, 512, 14, 14]               0\n",
      "            ReLU-128          [-1, 512, 14, 14]               0\n",
      "            ReLU-129          [-1, 512, 14, 14]               0\n",
      "            ReLU-130          [-1, 512, 14, 14]               0\n",
      "          Conv2d-131          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-132          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-133          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-134          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-135          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-136          [-1, 512, 14, 14]               0\n",
      "            ReLU-137          [-1, 512, 14, 14]               0\n",
      "            ReLU-138          [-1, 512, 14, 14]               0\n",
      "            ReLU-139          [-1, 512, 14, 14]               0\n",
      "            ReLU-140          [-1, 512, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-142          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-143          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-144          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-145          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-146          [-1, 512, 14, 14]               0\n",
      "            ReLU-147          [-1, 512, 14, 14]               0\n",
      "            ReLU-148          [-1, 512, 14, 14]               0\n",
      "            ReLU-149          [-1, 512, 14, 14]               0\n",
      "            ReLU-150          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-151            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-152            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-153            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-154            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-155            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-156            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-158            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-159            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 512, 7, 7]               0\n",
      "          Linear-161                 [-1, 4096]     102,764,544\n",
      "          Linear-162                 [-1, 4096]     102,764,544\n",
      "          Linear-163                 [-1, 4096]     102,764,544\n",
      "          Linear-164                 [-1, 4096]     102,764,544\n",
      "          Linear-165                 [-1, 4096]     102,764,544\n",
      "            ReLU-166                 [-1, 4096]               0\n",
      "            ReLU-167                 [-1, 4096]               0\n",
      "            ReLU-168                 [-1, 4096]               0\n",
      "            ReLU-169                 [-1, 4096]               0\n",
      "            ReLU-170                 [-1, 4096]               0\n",
      "         Dropout-171                 [-1, 4096]               0\n",
      "         Dropout-172                 [-1, 4096]               0\n",
      "         Dropout-173                 [-1, 4096]               0\n",
      "         Dropout-174                 [-1, 4096]               0\n",
      "         Dropout-175                 [-1, 4096]               0\n",
      "          Linear-176                 [-1, 4096]      16,781,312\n",
      "          Linear-177                 [-1, 4096]      16,781,312\n",
      "          Linear-178                 [-1, 4096]      16,781,312\n",
      "          Linear-179                 [-1, 4096]      16,781,312\n",
      "          Linear-180                 [-1, 4096]      16,781,312\n",
      "            ReLU-181                 [-1, 4096]               0\n",
      "            ReLU-182                 [-1, 4096]               0\n",
      "            ReLU-183                 [-1, 4096]               0\n",
      "            ReLU-184                 [-1, 4096]               0\n",
      "            ReLU-185                 [-1, 4096]               0\n",
      "         Dropout-186                 [-1, 4096]               0\n",
      "         Dropout-187                 [-1, 4096]               0\n",
      "         Dropout-188                 [-1, 4096]               0\n",
      "         Dropout-189                 [-1, 4096]               0\n",
      "         Dropout-190                 [-1, 4096]               0\n",
      "          Linear-191                   [-1, 16]          65,552\n",
      "          Linear-192                   [-1, 16]          65,552\n",
      "          Linear-193                   [-1, 16]          65,552\n",
      "          Linear-194                   [-1, 16]          65,552\n",
      "          Linear-195                   [-1, 16]          65,552\n",
      "             VGG-196                   [-1, 16]               0\n",
      "             VGG-197                   [-1, 16]               0\n",
      "             VGG-198                   [-1, 16]               0\n",
      "             VGG-199                   [-1, 16]               0\n",
      "             VGG-200                   [-1, 16]               0\n",
      "DocClassificationHolistic-201                   [-1, 16]               0\n",
      "DocClassificationHolistic-202                   [-1, 16]               0\n",
      "DocClassificationHolistic-203                   [-1, 16]               0\n",
      "DocClassificationHolistic-204                   [-1, 16]               0\n",
      "DocClassificationHolistic-205                   [-1, 16]               0\n",
      "          Conv2d-206         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-207         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-208         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-209         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-210         [-1, 64, 224, 224]           1,792\n",
      "            ReLU-211         [-1, 64, 224, 224]               0\n",
      "            ReLU-212         [-1, 64, 224, 224]               0\n",
      "            ReLU-213         [-1, 64, 224, 224]               0\n",
      "            ReLU-214         [-1, 64, 224, 224]               0\n",
      "            ReLU-215         [-1, 64, 224, 224]               0\n",
      "          Conv2d-216         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-217         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-218         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-219         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-220         [-1, 64, 224, 224]          36,928\n",
      "            ReLU-221         [-1, 64, 224, 224]               0\n",
      "            ReLU-222         [-1, 64, 224, 224]               0\n",
      "            ReLU-223         [-1, 64, 224, 224]               0\n",
      "            ReLU-224         [-1, 64, 224, 224]               0\n",
      "            ReLU-225         [-1, 64, 224, 224]               0\n",
      "       MaxPool2d-226         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-227         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-228         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-229         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-230         [-1, 64, 112, 112]               0\n",
      "          Conv2d-231        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-232        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-233        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-234        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-235        [-1, 128, 112, 112]          73,856\n",
      "            ReLU-236        [-1, 128, 112, 112]               0\n",
      "            ReLU-237        [-1, 128, 112, 112]               0\n",
      "            ReLU-238        [-1, 128, 112, 112]               0\n",
      "            ReLU-239        [-1, 128, 112, 112]               0\n",
      "            ReLU-240        [-1, 128, 112, 112]               0\n",
      "          Conv2d-241        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-242        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-243        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-244        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-245        [-1, 128, 112, 112]         147,584\n",
      "            ReLU-246        [-1, 128, 112, 112]               0\n",
      "            ReLU-247        [-1, 128, 112, 112]               0\n",
      "            ReLU-248        [-1, 128, 112, 112]               0\n",
      "            ReLU-249        [-1, 128, 112, 112]               0\n",
      "            ReLU-250        [-1, 128, 112, 112]               0\n",
      "       MaxPool2d-251          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-252          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-253          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-254          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-255          [-1, 128, 56, 56]               0\n",
      "          Conv2d-256          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-257          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-258          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-259          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-260          [-1, 256, 56, 56]         295,168\n",
      "            ReLU-261          [-1, 256, 56, 56]               0\n",
      "            ReLU-262          [-1, 256, 56, 56]               0\n",
      "            ReLU-263          [-1, 256, 56, 56]               0\n",
      "            ReLU-264          [-1, 256, 56, 56]               0\n",
      "            ReLU-265          [-1, 256, 56, 56]               0\n",
      "          Conv2d-266          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-267          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-268          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-269          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-270          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-271          [-1, 256, 56, 56]               0\n",
      "            ReLU-272          [-1, 256, 56, 56]               0\n",
      "            ReLU-273          [-1, 256, 56, 56]               0\n",
      "            ReLU-274          [-1, 256, 56, 56]               0\n",
      "            ReLU-275          [-1, 256, 56, 56]               0\n",
      "          Conv2d-276          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-277          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-278          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-279          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-280          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-281          [-1, 256, 56, 56]               0\n",
      "            ReLU-282          [-1, 256, 56, 56]               0\n",
      "            ReLU-283          [-1, 256, 56, 56]               0\n",
      "            ReLU-284          [-1, 256, 56, 56]               0\n",
      "            ReLU-285          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-286          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-287          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-288          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-289          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-290          [-1, 256, 28, 28]               0\n",
      "          Conv2d-291          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-292          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-293          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-294          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-295          [-1, 512, 28, 28]       1,180,160\n",
      "            ReLU-296          [-1, 512, 28, 28]               0\n",
      "            ReLU-297          [-1, 512, 28, 28]               0\n",
      "            ReLU-298          [-1, 512, 28, 28]               0\n",
      "            ReLU-299          [-1, 512, 28, 28]               0\n",
      "            ReLU-300          [-1, 512, 28, 28]               0\n",
      "          Conv2d-301          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-302          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-303          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-304          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-305          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-306          [-1, 512, 28, 28]               0\n",
      "            ReLU-307          [-1, 512, 28, 28]               0\n",
      "            ReLU-308          [-1, 512, 28, 28]               0\n",
      "            ReLU-309          [-1, 512, 28, 28]               0\n",
      "            ReLU-310          [-1, 512, 28, 28]               0\n",
      "          Conv2d-311          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-312          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-313          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-314          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-315          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-316          [-1, 512, 28, 28]               0\n",
      "            ReLU-317          [-1, 512, 28, 28]               0\n",
      "            ReLU-318          [-1, 512, 28, 28]               0\n",
      "            ReLU-319          [-1, 512, 28, 28]               0\n",
      "            ReLU-320          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-321          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-322          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-323          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-324          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-325          [-1, 512, 14, 14]               0\n",
      "          Conv2d-326          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-327          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-328          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-329          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-330          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-331          [-1, 512, 14, 14]               0\n",
      "            ReLU-332          [-1, 512, 14, 14]               0\n",
      "            ReLU-333          [-1, 512, 14, 14]               0\n",
      "            ReLU-334          [-1, 512, 14, 14]               0\n",
      "            ReLU-335          [-1, 512, 14, 14]               0\n",
      "          Conv2d-336          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-337          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-338          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-339          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-340          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-341          [-1, 512, 14, 14]               0\n",
      "            ReLU-342          [-1, 512, 14, 14]               0\n",
      "            ReLU-343          [-1, 512, 14, 14]               0\n",
      "            ReLU-344          [-1, 512, 14, 14]               0\n",
      "            ReLU-345          [-1, 512, 14, 14]               0\n",
      "          Conv2d-346          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-347          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-348          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-349          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-350          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-351          [-1, 512, 14, 14]               0\n",
      "            ReLU-352          [-1, 512, 14, 14]               0\n",
      "            ReLU-353          [-1, 512, 14, 14]               0\n",
      "            ReLU-354          [-1, 512, 14, 14]               0\n",
      "            ReLU-355          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-356            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-357            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-358            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-359            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-360            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-361            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-362            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-363            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-364            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-365            [-1, 512, 7, 7]               0\n",
      "          Linear-366                 [-1, 4096]     102,764,544\n",
      "          Linear-367                 [-1, 4096]     102,764,544\n",
      "          Linear-368                 [-1, 4096]     102,764,544\n",
      "          Linear-369                 [-1, 4096]     102,764,544\n",
      "          Linear-370                 [-1, 4096]     102,764,544\n",
      "            ReLU-371                 [-1, 4096]               0\n",
      "            ReLU-372                 [-1, 4096]               0\n",
      "            ReLU-373                 [-1, 4096]               0\n",
      "            ReLU-374                 [-1, 4096]               0\n",
      "            ReLU-375                 [-1, 4096]               0\n",
      "         Dropout-376                 [-1, 4096]               0\n",
      "         Dropout-377                 [-1, 4096]               0\n",
      "         Dropout-378                 [-1, 4096]               0\n",
      "         Dropout-379                 [-1, 4096]               0\n",
      "         Dropout-380                 [-1, 4096]               0\n",
      "          Linear-381                 [-1, 4096]      16,781,312\n",
      "          Linear-382                 [-1, 4096]      16,781,312\n",
      "          Linear-383                 [-1, 4096]      16,781,312\n",
      "          Linear-384                 [-1, 4096]      16,781,312\n",
      "          Linear-385                 [-1, 4096]      16,781,312\n",
      "            ReLU-386                 [-1, 4096]               0\n",
      "            ReLU-387                 [-1, 4096]               0\n",
      "            ReLU-388                 [-1, 4096]               0\n",
      "            ReLU-389                 [-1, 4096]               0\n",
      "            ReLU-390                 [-1, 4096]               0\n",
      "         Dropout-391                 [-1, 4096]               0\n",
      "         Dropout-392                 [-1, 4096]               0\n",
      "         Dropout-393                 [-1, 4096]               0\n",
      "         Dropout-394                 [-1, 4096]               0\n",
      "         Dropout-395                 [-1, 4096]               0\n",
      "          Linear-396                   [-1, 16]          65,552\n",
      "          Linear-397                   [-1, 16]          65,552\n",
      "          Linear-398                   [-1, 16]          65,552\n",
      "          Linear-399                   [-1, 16]          65,552\n",
      "          Linear-400                   [-1, 16]          65,552\n",
      "             VGG-401                   [-1, 16]               0\n",
      "             VGG-402                   [-1, 16]               0\n",
      "             VGG-403                   [-1, 16]               0\n",
      "             VGG-404                   [-1, 16]               0\n",
      "             VGG-405                   [-1, 16]               0\n",
      "DocClassificationHolistic-406                   [-1, 16]               0\n",
      "DocClassificationHolistic-407                   [-1, 16]               0\n",
      "DocClassificationHolistic-408                   [-1, 16]               0\n",
      "DocClassificationHolistic-409                   [-1, 16]               0\n",
      "DocClassificationHolistic-410                   [-1, 16]               0\n",
      "DocClassificationRest-411                   [-1, 16]               0\n",
      "          Conv2d-412         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-413         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-414         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-415         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-416         [-1, 64, 224, 224]           1,792\n",
      "            ReLU-417         [-1, 64, 224, 224]               0\n",
      "            ReLU-418         [-1, 64, 224, 224]               0\n",
      "            ReLU-419         [-1, 64, 224, 224]               0\n",
      "            ReLU-420         [-1, 64, 224, 224]               0\n",
      "            ReLU-421         [-1, 64, 224, 224]               0\n",
      "          Conv2d-422         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-423         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-424         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-425         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-426         [-1, 64, 224, 224]          36,928\n",
      "            ReLU-427         [-1, 64, 224, 224]               0\n",
      "            ReLU-428         [-1, 64, 224, 224]               0\n",
      "            ReLU-429         [-1, 64, 224, 224]               0\n",
      "            ReLU-430         [-1, 64, 224, 224]               0\n",
      "            ReLU-431         [-1, 64, 224, 224]               0\n",
      "       MaxPool2d-432         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-433         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-434         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-435         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-436         [-1, 64, 112, 112]               0\n",
      "          Conv2d-437        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-438        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-439        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-440        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-441        [-1, 128, 112, 112]          73,856\n",
      "            ReLU-442        [-1, 128, 112, 112]               0\n",
      "            ReLU-443        [-1, 128, 112, 112]               0\n",
      "            ReLU-444        [-1, 128, 112, 112]               0\n",
      "            ReLU-445        [-1, 128, 112, 112]               0\n",
      "            ReLU-446        [-1, 128, 112, 112]               0\n",
      "          Conv2d-447        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-448        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-449        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-450        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-451        [-1, 128, 112, 112]         147,584\n",
      "            ReLU-452        [-1, 128, 112, 112]               0\n",
      "            ReLU-453        [-1, 128, 112, 112]               0\n",
      "            ReLU-454        [-1, 128, 112, 112]               0\n",
      "            ReLU-455        [-1, 128, 112, 112]               0\n",
      "            ReLU-456        [-1, 128, 112, 112]               0\n",
      "       MaxPool2d-457          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-458          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-459          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-460          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-461          [-1, 128, 56, 56]               0\n",
      "          Conv2d-462          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-463          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-464          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-465          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-466          [-1, 256, 56, 56]         295,168\n",
      "            ReLU-467          [-1, 256, 56, 56]               0\n",
      "            ReLU-468          [-1, 256, 56, 56]               0\n",
      "            ReLU-469          [-1, 256, 56, 56]               0\n",
      "            ReLU-470          [-1, 256, 56, 56]               0\n",
      "            ReLU-471          [-1, 256, 56, 56]               0\n",
      "          Conv2d-472          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-473          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-474          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-475          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-476          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-477          [-1, 256, 56, 56]               0\n",
      "            ReLU-478          [-1, 256, 56, 56]               0\n",
      "            ReLU-479          [-1, 256, 56, 56]               0\n",
      "            ReLU-480          [-1, 256, 56, 56]               0\n",
      "            ReLU-481          [-1, 256, 56, 56]               0\n",
      "          Conv2d-482          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-483          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-484          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-485          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-486          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-487          [-1, 256, 56, 56]               0\n",
      "            ReLU-488          [-1, 256, 56, 56]               0\n",
      "            ReLU-489          [-1, 256, 56, 56]               0\n",
      "            ReLU-490          [-1, 256, 56, 56]               0\n",
      "            ReLU-491          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-492          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-493          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-494          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-495          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-496          [-1, 256, 28, 28]               0\n",
      "          Conv2d-497          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-498          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-499          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-500          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-501          [-1, 512, 28, 28]       1,180,160\n",
      "            ReLU-502          [-1, 512, 28, 28]               0\n",
      "            ReLU-503          [-1, 512, 28, 28]               0\n",
      "            ReLU-504          [-1, 512, 28, 28]               0\n",
      "            ReLU-505          [-1, 512, 28, 28]               0\n",
      "            ReLU-506          [-1, 512, 28, 28]               0\n",
      "          Conv2d-507          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-508          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-509          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-510          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-511          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-512          [-1, 512, 28, 28]               0\n",
      "            ReLU-513          [-1, 512, 28, 28]               0\n",
      "            ReLU-514          [-1, 512, 28, 28]               0\n",
      "            ReLU-515          [-1, 512, 28, 28]               0\n",
      "            ReLU-516          [-1, 512, 28, 28]               0\n",
      "          Conv2d-517          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-518          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-519          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-520          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-521          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-522          [-1, 512, 28, 28]               0\n",
      "            ReLU-523          [-1, 512, 28, 28]               0\n",
      "            ReLU-524          [-1, 512, 28, 28]               0\n",
      "            ReLU-525          [-1, 512, 28, 28]               0\n",
      "            ReLU-526          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-527          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-528          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-529          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-530          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-531          [-1, 512, 14, 14]               0\n",
      "          Conv2d-532          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-533          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-534          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-535          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-536          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-537          [-1, 512, 14, 14]               0\n",
      "            ReLU-538          [-1, 512, 14, 14]               0\n",
      "            ReLU-539          [-1, 512, 14, 14]               0\n",
      "            ReLU-540          [-1, 512, 14, 14]               0\n",
      "            ReLU-541          [-1, 512, 14, 14]               0\n",
      "          Conv2d-542          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-543          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-544          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-545          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-546          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-547          [-1, 512, 14, 14]               0\n",
      "            ReLU-548          [-1, 512, 14, 14]               0\n",
      "            ReLU-549          [-1, 512, 14, 14]               0\n",
      "            ReLU-550          [-1, 512, 14, 14]               0\n",
      "            ReLU-551          [-1, 512, 14, 14]               0\n",
      "          Conv2d-552          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-553          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-554          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-555          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-556          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-557          [-1, 512, 14, 14]               0\n",
      "            ReLU-558          [-1, 512, 14, 14]               0\n",
      "            ReLU-559          [-1, 512, 14, 14]               0\n",
      "            ReLU-560          [-1, 512, 14, 14]               0\n",
      "            ReLU-561          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-562            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-563            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-564            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-565            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-566            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-567            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-568            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-569            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-570            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-571            [-1, 512, 7, 7]               0\n",
      "          Linear-572                 [-1, 4096]     102,764,544\n",
      "          Linear-573                 [-1, 4096]     102,764,544\n",
      "          Linear-574                 [-1, 4096]     102,764,544\n",
      "          Linear-575                 [-1, 4096]     102,764,544\n",
      "          Linear-576                 [-1, 4096]     102,764,544\n",
      "            ReLU-577                 [-1, 4096]               0\n",
      "            ReLU-578                 [-1, 4096]               0\n",
      "            ReLU-579                 [-1, 4096]               0\n",
      "            ReLU-580                 [-1, 4096]               0\n",
      "            ReLU-581                 [-1, 4096]               0\n",
      "         Dropout-582                 [-1, 4096]               0\n",
      "         Dropout-583                 [-1, 4096]               0\n",
      "         Dropout-584                 [-1, 4096]               0\n",
      "         Dropout-585                 [-1, 4096]               0\n",
      "         Dropout-586                 [-1, 4096]               0\n",
      "          Linear-587                 [-1, 4096]      16,781,312\n",
      "          Linear-588                 [-1, 4096]      16,781,312\n",
      "          Linear-589                 [-1, 4096]      16,781,312\n",
      "          Linear-590                 [-1, 4096]      16,781,312\n",
      "          Linear-591                 [-1, 4096]      16,781,312\n",
      "            ReLU-592                 [-1, 4096]               0\n",
      "            ReLU-593                 [-1, 4096]               0\n",
      "            ReLU-594                 [-1, 4096]               0\n",
      "            ReLU-595                 [-1, 4096]               0\n",
      "            ReLU-596                 [-1, 4096]               0\n",
      "         Dropout-597                 [-1, 4096]               0\n",
      "         Dropout-598                 [-1, 4096]               0\n",
      "         Dropout-599                 [-1, 4096]               0\n",
      "         Dropout-600                 [-1, 4096]               0\n",
      "         Dropout-601                 [-1, 4096]               0\n",
      "          Linear-602                   [-1, 16]          65,552\n",
      "          Linear-603                   [-1, 16]          65,552\n",
      "          Linear-604                   [-1, 16]          65,552\n",
      "          Linear-605                   [-1, 16]          65,552\n",
      "          Linear-606                   [-1, 16]          65,552\n",
      "             VGG-607                   [-1, 16]               0\n",
      "             VGG-608                   [-1, 16]               0\n",
      "             VGG-609                   [-1, 16]               0\n",
      "             VGG-610                   [-1, 16]               0\n",
      "             VGG-611                   [-1, 16]               0\n",
      "DocClassificationHolistic-612                   [-1, 16]               0\n",
      "DocClassificationHolistic-613                   [-1, 16]               0\n",
      "DocClassificationHolistic-614                   [-1, 16]               0\n",
      "DocClassificationHolistic-615                   [-1, 16]               0\n",
      "DocClassificationHolistic-616                   [-1, 16]               0\n",
      "DocClassificationRest-617                   [-1, 16]               0\n",
      "          Conv2d-618         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-619         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-620         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-621         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-622         [-1, 64, 224, 224]           1,792\n",
      "            ReLU-623         [-1, 64, 224, 224]               0\n",
      "            ReLU-624         [-1, 64, 224, 224]               0\n",
      "            ReLU-625         [-1, 64, 224, 224]               0\n",
      "            ReLU-626         [-1, 64, 224, 224]               0\n",
      "            ReLU-627         [-1, 64, 224, 224]               0\n",
      "          Conv2d-628         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-629         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-630         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-631         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-632         [-1, 64, 224, 224]          36,928\n",
      "            ReLU-633         [-1, 64, 224, 224]               0\n",
      "            ReLU-634         [-1, 64, 224, 224]               0\n",
      "            ReLU-635         [-1, 64, 224, 224]               0\n",
      "            ReLU-636         [-1, 64, 224, 224]               0\n",
      "            ReLU-637         [-1, 64, 224, 224]               0\n",
      "       MaxPool2d-638         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-639         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-640         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-641         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-642         [-1, 64, 112, 112]               0\n",
      "          Conv2d-643        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-644        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-645        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-646        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-647        [-1, 128, 112, 112]          73,856\n",
      "            ReLU-648        [-1, 128, 112, 112]               0\n",
      "            ReLU-649        [-1, 128, 112, 112]               0\n",
      "            ReLU-650        [-1, 128, 112, 112]               0\n",
      "            ReLU-651        [-1, 128, 112, 112]               0\n",
      "            ReLU-652        [-1, 128, 112, 112]               0\n",
      "          Conv2d-653        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-654        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-655        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-656        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-657        [-1, 128, 112, 112]         147,584\n",
      "            ReLU-658        [-1, 128, 112, 112]               0\n",
      "            ReLU-659        [-1, 128, 112, 112]               0\n",
      "            ReLU-660        [-1, 128, 112, 112]               0\n",
      "            ReLU-661        [-1, 128, 112, 112]               0\n",
      "            ReLU-662        [-1, 128, 112, 112]               0\n",
      "       MaxPool2d-663          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-664          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-665          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-666          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-667          [-1, 128, 56, 56]               0\n",
      "          Conv2d-668          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-669          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-670          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-671          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-672          [-1, 256, 56, 56]         295,168\n",
      "            ReLU-673          [-1, 256, 56, 56]               0\n",
      "            ReLU-674          [-1, 256, 56, 56]               0\n",
      "            ReLU-675          [-1, 256, 56, 56]               0\n",
      "            ReLU-676          [-1, 256, 56, 56]               0\n",
      "            ReLU-677          [-1, 256, 56, 56]               0\n",
      "          Conv2d-678          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-679          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-680          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-681          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-682          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-683          [-1, 256, 56, 56]               0\n",
      "            ReLU-684          [-1, 256, 56, 56]               0\n",
      "            ReLU-685          [-1, 256, 56, 56]               0\n",
      "            ReLU-686          [-1, 256, 56, 56]               0\n",
      "            ReLU-687          [-1, 256, 56, 56]               0\n",
      "          Conv2d-688          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-689          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-690          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-691          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-692          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-693          [-1, 256, 56, 56]               0\n",
      "            ReLU-694          [-1, 256, 56, 56]               0\n",
      "            ReLU-695          [-1, 256, 56, 56]               0\n",
      "            ReLU-696          [-1, 256, 56, 56]               0\n",
      "            ReLU-697          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-698          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-699          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-700          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-701          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-702          [-1, 256, 28, 28]               0\n",
      "          Conv2d-703          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-704          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-705          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-706          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-707          [-1, 512, 28, 28]       1,180,160\n",
      "            ReLU-708          [-1, 512, 28, 28]               0\n",
      "            ReLU-709          [-1, 512, 28, 28]               0\n",
      "            ReLU-710          [-1, 512, 28, 28]               0\n",
      "            ReLU-711          [-1, 512, 28, 28]               0\n",
      "            ReLU-712          [-1, 512, 28, 28]               0\n",
      "          Conv2d-713          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-714          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-715          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-716          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-717          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-718          [-1, 512, 28, 28]               0\n",
      "            ReLU-719          [-1, 512, 28, 28]               0\n",
      "            ReLU-720          [-1, 512, 28, 28]               0\n",
      "            ReLU-721          [-1, 512, 28, 28]               0\n",
      "            ReLU-722          [-1, 512, 28, 28]               0\n",
      "          Conv2d-723          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-724          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-725          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-726          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-727          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-728          [-1, 512, 28, 28]               0\n",
      "            ReLU-729          [-1, 512, 28, 28]               0\n",
      "            ReLU-730          [-1, 512, 28, 28]               0\n",
      "            ReLU-731          [-1, 512, 28, 28]               0\n",
      "            ReLU-732          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-733          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-734          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-735          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-736          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-737          [-1, 512, 14, 14]               0\n",
      "          Conv2d-738          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-739          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-740          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-741          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-742          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-743          [-1, 512, 14, 14]               0\n",
      "            ReLU-744          [-1, 512, 14, 14]               0\n",
      "            ReLU-745          [-1, 512, 14, 14]               0\n",
      "            ReLU-746          [-1, 512, 14, 14]               0\n",
      "            ReLU-747          [-1, 512, 14, 14]               0\n",
      "          Conv2d-748          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-749          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-750          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-751          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-752          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-753          [-1, 512, 14, 14]               0\n",
      "            ReLU-754          [-1, 512, 14, 14]               0\n",
      "            ReLU-755          [-1, 512, 14, 14]               0\n",
      "            ReLU-756          [-1, 512, 14, 14]               0\n",
      "            ReLU-757          [-1, 512, 14, 14]               0\n",
      "          Conv2d-758          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-759          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-760          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-761          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-762          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-763          [-1, 512, 14, 14]               0\n",
      "            ReLU-764          [-1, 512, 14, 14]               0\n",
      "            ReLU-765          [-1, 512, 14, 14]               0\n",
      "            ReLU-766          [-1, 512, 14, 14]               0\n",
      "            ReLU-767          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-768            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-769            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-770            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-771            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-772            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-773            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-774            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-775            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-776            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-777            [-1, 512, 7, 7]               0\n",
      "          Linear-778                 [-1, 4096]     102,764,544\n",
      "          Linear-779                 [-1, 4096]     102,764,544\n",
      "          Linear-780                 [-1, 4096]     102,764,544\n",
      "          Linear-781                 [-1, 4096]     102,764,544\n",
      "          Linear-782                 [-1, 4096]     102,764,544\n",
      "            ReLU-783                 [-1, 4096]               0\n",
      "            ReLU-784                 [-1, 4096]               0\n",
      "            ReLU-785                 [-1, 4096]               0\n",
      "            ReLU-786                 [-1, 4096]               0\n",
      "            ReLU-787                 [-1, 4096]               0\n",
      "         Dropout-788                 [-1, 4096]               0\n",
      "         Dropout-789                 [-1, 4096]               0\n",
      "         Dropout-790                 [-1, 4096]               0\n",
      "         Dropout-791                 [-1, 4096]               0\n",
      "         Dropout-792                 [-1, 4096]               0\n",
      "          Linear-793                 [-1, 4096]      16,781,312\n",
      "          Linear-794                 [-1, 4096]      16,781,312\n",
      "          Linear-795                 [-1, 4096]      16,781,312\n",
      "          Linear-796                 [-1, 4096]      16,781,312\n",
      "          Linear-797                 [-1, 4096]      16,781,312\n",
      "            ReLU-798                 [-1, 4096]               0\n",
      "            ReLU-799                 [-1, 4096]               0\n",
      "            ReLU-800                 [-1, 4096]               0\n",
      "            ReLU-801                 [-1, 4096]               0\n",
      "            ReLU-802                 [-1, 4096]               0\n",
      "         Dropout-803                 [-1, 4096]               0\n",
      "         Dropout-804                 [-1, 4096]               0\n",
      "         Dropout-805                 [-1, 4096]               0\n",
      "         Dropout-806                 [-1, 4096]               0\n",
      "         Dropout-807                 [-1, 4096]               0\n",
      "          Linear-808                   [-1, 16]          65,552\n",
      "          Linear-809                   [-1, 16]          65,552\n",
      "          Linear-810                   [-1, 16]          65,552\n",
      "          Linear-811                   [-1, 16]          65,552\n",
      "          Linear-812                   [-1, 16]          65,552\n",
      "             VGG-813                   [-1, 16]               0\n",
      "             VGG-814                   [-1, 16]               0\n",
      "             VGG-815                   [-1, 16]               0\n",
      "             VGG-816                   [-1, 16]               0\n",
      "             VGG-817                   [-1, 16]               0\n",
      "DocClassificationHolistic-818                   [-1, 16]               0\n",
      "DocClassificationHolistic-819                   [-1, 16]               0\n",
      "DocClassificationHolistic-820                   [-1, 16]               0\n",
      "DocClassificationHolistic-821                   [-1, 16]               0\n",
      "DocClassificationHolistic-822                   [-1, 16]               0\n",
      "DocClassificationRest-823                   [-1, 16]               0\n",
      "          Conv2d-824         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-825         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-826         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-827         [-1, 64, 224, 224]           1,792\n",
      "          Conv2d-828         [-1, 64, 224, 224]           1,792\n",
      "            ReLU-829         [-1, 64, 224, 224]               0\n",
      "            ReLU-830         [-1, 64, 224, 224]               0\n",
      "            ReLU-831         [-1, 64, 224, 224]               0\n",
      "            ReLU-832         [-1, 64, 224, 224]               0\n",
      "            ReLU-833         [-1, 64, 224, 224]               0\n",
      "          Conv2d-834         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-835         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-836         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-837         [-1, 64, 224, 224]          36,928\n",
      "          Conv2d-838         [-1, 64, 224, 224]          36,928\n",
      "            ReLU-839         [-1, 64, 224, 224]               0\n",
      "            ReLU-840         [-1, 64, 224, 224]               0\n",
      "            ReLU-841         [-1, 64, 224, 224]               0\n",
      "            ReLU-842         [-1, 64, 224, 224]               0\n",
      "            ReLU-843         [-1, 64, 224, 224]               0\n",
      "       MaxPool2d-844         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-845         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-846         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-847         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-848         [-1, 64, 112, 112]               0\n",
      "          Conv2d-849        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-850        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-851        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-852        [-1, 128, 112, 112]          73,856\n",
      "          Conv2d-853        [-1, 128, 112, 112]          73,856\n",
      "            ReLU-854        [-1, 128, 112, 112]               0\n",
      "            ReLU-855        [-1, 128, 112, 112]               0\n",
      "            ReLU-856        [-1, 128, 112, 112]               0\n",
      "            ReLU-857        [-1, 128, 112, 112]               0\n",
      "            ReLU-858        [-1, 128, 112, 112]               0\n",
      "          Conv2d-859        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-860        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-861        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-862        [-1, 128, 112, 112]         147,584\n",
      "          Conv2d-863        [-1, 128, 112, 112]         147,584\n",
      "            ReLU-864        [-1, 128, 112, 112]               0\n",
      "            ReLU-865        [-1, 128, 112, 112]               0\n",
      "            ReLU-866        [-1, 128, 112, 112]               0\n",
      "            ReLU-867        [-1, 128, 112, 112]               0\n",
      "            ReLU-868        [-1, 128, 112, 112]               0\n",
      "       MaxPool2d-869          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-870          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-871          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-872          [-1, 128, 56, 56]               0\n",
      "       MaxPool2d-873          [-1, 128, 56, 56]               0\n",
      "          Conv2d-874          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-875          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-876          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-877          [-1, 256, 56, 56]         295,168\n",
      "          Conv2d-878          [-1, 256, 56, 56]         295,168\n",
      "            ReLU-879          [-1, 256, 56, 56]               0\n",
      "            ReLU-880          [-1, 256, 56, 56]               0\n",
      "            ReLU-881          [-1, 256, 56, 56]               0\n",
      "            ReLU-882          [-1, 256, 56, 56]               0\n",
      "            ReLU-883          [-1, 256, 56, 56]               0\n",
      "          Conv2d-884          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-885          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-886          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-887          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-888          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-889          [-1, 256, 56, 56]               0\n",
      "            ReLU-890          [-1, 256, 56, 56]               0\n",
      "            ReLU-891          [-1, 256, 56, 56]               0\n",
      "            ReLU-892          [-1, 256, 56, 56]               0\n",
      "            ReLU-893          [-1, 256, 56, 56]               0\n",
      "          Conv2d-894          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-895          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-896          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-897          [-1, 256, 56, 56]         590,080\n",
      "          Conv2d-898          [-1, 256, 56, 56]         590,080\n",
      "            ReLU-899          [-1, 256, 56, 56]               0\n",
      "            ReLU-900          [-1, 256, 56, 56]               0\n",
      "            ReLU-901          [-1, 256, 56, 56]               0\n",
      "            ReLU-902          [-1, 256, 56, 56]               0\n",
      "            ReLU-903          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-904          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-905          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-906          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-907          [-1, 256, 28, 28]               0\n",
      "       MaxPool2d-908          [-1, 256, 28, 28]               0\n",
      "          Conv2d-909          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-910          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-911          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-912          [-1, 512, 28, 28]       1,180,160\n",
      "          Conv2d-913          [-1, 512, 28, 28]       1,180,160\n",
      "            ReLU-914          [-1, 512, 28, 28]               0\n",
      "            ReLU-915          [-1, 512, 28, 28]               0\n",
      "            ReLU-916          [-1, 512, 28, 28]               0\n",
      "            ReLU-917          [-1, 512, 28, 28]               0\n",
      "            ReLU-918          [-1, 512, 28, 28]               0\n",
      "          Conv2d-919          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-920          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-921          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-922          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-923          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-924          [-1, 512, 28, 28]               0\n",
      "            ReLU-925          [-1, 512, 28, 28]               0\n",
      "            ReLU-926          [-1, 512, 28, 28]               0\n",
      "            ReLU-927          [-1, 512, 28, 28]               0\n",
      "            ReLU-928          [-1, 512, 28, 28]               0\n",
      "          Conv2d-929          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-930          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-931          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-932          [-1, 512, 28, 28]       2,359,808\n",
      "          Conv2d-933          [-1, 512, 28, 28]       2,359,808\n",
      "            ReLU-934          [-1, 512, 28, 28]               0\n",
      "            ReLU-935          [-1, 512, 28, 28]               0\n",
      "            ReLU-936          [-1, 512, 28, 28]               0\n",
      "            ReLU-937          [-1, 512, 28, 28]               0\n",
      "            ReLU-938          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-939          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-940          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-941          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-942          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-943          [-1, 512, 14, 14]               0\n",
      "          Conv2d-944          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-945          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-946          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-947          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-948          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-949          [-1, 512, 14, 14]               0\n",
      "            ReLU-950          [-1, 512, 14, 14]               0\n",
      "            ReLU-951          [-1, 512, 14, 14]               0\n",
      "            ReLU-952          [-1, 512, 14, 14]               0\n",
      "            ReLU-953          [-1, 512, 14, 14]               0\n",
      "          Conv2d-954          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-955          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-956          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-957          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-958          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-959          [-1, 512, 14, 14]               0\n",
      "            ReLU-960          [-1, 512, 14, 14]               0\n",
      "            ReLU-961          [-1, 512, 14, 14]               0\n",
      "            ReLU-962          [-1, 512, 14, 14]               0\n",
      "            ReLU-963          [-1, 512, 14, 14]               0\n",
      "          Conv2d-964          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-965          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-966          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-967          [-1, 512, 14, 14]       2,359,808\n",
      "          Conv2d-968          [-1, 512, 14, 14]       2,359,808\n",
      "            ReLU-969          [-1, 512, 14, 14]               0\n",
      "            ReLU-970          [-1, 512, 14, 14]               0\n",
      "            ReLU-971          [-1, 512, 14, 14]               0\n",
      "            ReLU-972          [-1, 512, 14, 14]               0\n",
      "            ReLU-973          [-1, 512, 14, 14]               0\n",
      "       MaxPool2d-974            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-975            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-976            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-977            [-1, 512, 7, 7]               0\n",
      "       MaxPool2d-978            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-979            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-980            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-981            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-982            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-983            [-1, 512, 7, 7]               0\n",
      "          Linear-984                 [-1, 4096]     102,764,544\n",
      "          Linear-985                 [-1, 4096]     102,764,544\n",
      "          Linear-986                 [-1, 4096]     102,764,544\n",
      "          Linear-987                 [-1, 4096]     102,764,544\n",
      "          Linear-988                 [-1, 4096]     102,764,544\n",
      "            ReLU-989                 [-1, 4096]               0\n",
      "            ReLU-990                 [-1, 4096]               0\n",
      "            ReLU-991                 [-1, 4096]               0\n",
      "            ReLU-992                 [-1, 4096]               0\n",
      "            ReLU-993                 [-1, 4096]               0\n",
      "         Dropout-994                 [-1, 4096]               0\n",
      "         Dropout-995                 [-1, 4096]               0\n",
      "         Dropout-996                 [-1, 4096]               0\n",
      "         Dropout-997                 [-1, 4096]               0\n",
      "         Dropout-998                 [-1, 4096]               0\n",
      "          Linear-999                 [-1, 4096]      16,781,312\n",
      "         Linear-1000                 [-1, 4096]      16,781,312\n",
      "         Linear-1001                 [-1, 4096]      16,781,312\n",
      "         Linear-1002                 [-1, 4096]      16,781,312\n",
      "         Linear-1003                 [-1, 4096]      16,781,312\n",
      "           ReLU-1004                 [-1, 4096]               0\n",
      "           ReLU-1005                 [-1, 4096]               0\n",
      "           ReLU-1006                 [-1, 4096]               0\n",
      "           ReLU-1007                 [-1, 4096]               0\n",
      "           ReLU-1008                 [-1, 4096]               0\n",
      "        Dropout-1009                 [-1, 4096]               0\n",
      "        Dropout-1010                 [-1, 4096]               0\n",
      "        Dropout-1011                 [-1, 4096]               0\n",
      "        Dropout-1012                 [-1, 4096]               0\n",
      "        Dropout-1013                 [-1, 4096]               0\n",
      "         Linear-1014                   [-1, 16]          65,552\n",
      "         Linear-1015                   [-1, 16]          65,552\n",
      "         Linear-1016                   [-1, 16]          65,552\n",
      "         Linear-1017                   [-1, 16]          65,552\n",
      "         Linear-1018                   [-1, 16]          65,552\n",
      "            VGG-1019                   [-1, 16]               0\n",
      "            VGG-1020                   [-1, 16]               0\n",
      "            VGG-1021                   [-1, 16]               0\n",
      "            VGG-1022                   [-1, 16]               0\n",
      "            VGG-1023                   [-1, 16]               0\n",
      "DocClassificationHolistic-1024                   [-1, 16]               0\n",
      "DocClassificationHolistic-1025                   [-1, 16]               0\n",
      "DocClassificationHolistic-1026                   [-1, 16]               0\n",
      "DocClassificationHolistic-1027                   [-1, 16]               0\n",
      "DocClassificationHolistic-1028                   [-1, 16]               0\n",
      "DocClassificationRest-1029                   [-1, 16]               0\n",
      "         Linear-1030                  [-1, 256]          20,736\n",
      "         Linear-1031                  [-1, 256]          65,792\n",
      "         Linear-1032                   [-1, 16]           4,112\n",
      "================================================================\n",
      "Total params: 3,358,243,040\n",
      "Trainable params: 90,640\n",
      "Non-trainable params: 3,358,152,400\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 14555644166144.00\n",
      "Forward/backward pass size (MB): 5469.35\n",
      "Params size (MB): 12810.68\n",
      "Estimated Total Size (MB): 14555644184424.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bh,h,f,l,r,lab = test_dataset[np.random.randint(0,16000,2)]\n",
    "batch_holistic = torch.unsqueeze(bh , dim=1).repeat(1,3,1,1).float().cuda()\n",
    "bh = np.array(batch_holistic.detach().cpu())\n",
    "# print(bh.shape, batch_holistic.shape)\n",
    "batch_header = torch.unsqueeze(h, dim=1).repeat(1,3,1,1).float().cuda()\n",
    "batch_footer = torch.unsqueeze(f, dim=1).repeat(1,3,1,1).float().cuda()\n",
    "batch_left = torch.unsqueeze(l, dim=1).repeat(1,3,1,1).float().cuda()\n",
    "batch_right = torch.unsqueeze(r, dim=1).repeat(1,3,1,1).float().cuda()\n",
    "\n",
    "# all_data = [batch_holistic,batch_header,batch_footer,batch_left,batch_right]\n",
    "# writer.add_graph(model, input_to_model=all_data)\n",
    "# writer.flush()\n",
    "summary(model,[bh.shape[1:],batch_header.shape[1:],batch_footer.shape[1:],batch_left.shape[1:],batch_right.shape[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# def plot_data(x,ax):\n",
    "# #     x = x.reshape((size,size))\n",
    "    \n",
    "#     ax.imshow(x, cmap='gray')\n",
    "# #     if y is not None:\n",
    "# #         ax.scatter(y[0::2] , y[1::2] , marker='x', s=10)\n",
    "# def plot_images(x,batch_size=64):\n",
    "#     fig = plt.figure(figsize=(50,50))\n",
    "#     fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.01, wspace=0.01)\n",
    "#     for i in range(batch_size):\n",
    "#         ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "#     #     print(y['classes'][i])\n",
    "#     #     print(np.argmax(c[i]))\n",
    "#         plot_data(x[i], ax)\n",
    "# plot_images(bh)\n",
    "# print(l)\n",
    "# # display(plot_images(np.squeeze(seg[0],axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # start_point = 0\n",
    "# # early_stop_count = 0\n",
    "# for epoch in tqdm(range(start_point, args.epochs),desc='Training:'):\n",
    "\n",
    "#     train_loss = 0.0\n",
    "#     dev_loss = 0.0\n",
    "#     test_loss = 0.0\n",
    "\n",
    "#     train_predictions = []\n",
    "#     train_labels = []\n",
    "\n",
    "#     dev_predictions = []\n",
    "#     dev_labels = []\n",
    "\n",
    "#     test_predictions = []\n",
    "#     test_labels = []\n",
    "\n",
    "    \n",
    "\n",
    "#     batch_train_data = list(np.random.randint(0, len(train_idx), args.batchsize))\n",
    "#     batch_dev_data = list(np.random.randint(0, len(dev_idx), args.batchsize))\n",
    "#     batch_test_data = list(np.random.randint(0, len(test_idx), args.batchsize))\n",
    "    \n",
    "# # #     for batch in tqdm(batch_train_data, desc='Training batches..'):\n",
    "# #     # ----------------- Train ---------------------\n",
    "# #     train_batch_holistic, \\\n",
    "# #             train_batch_header, \\\n",
    "# #             train_batch_footer, \\\n",
    "# #             train_batch_left_body, \\\n",
    "# #             train_batch_right_body, \\\n",
    "# #             train_batch_labels = train_dataset[batch_train_data]\n",
    "# # #     print(train_batch_holistic.shape, \\\n",
    "# # #             train_batch_header.shape, \\\n",
    "# # #             train_batch_footer.shape, \\\n",
    "# # #             train_batch_left_body.shape, \\\n",
    "# # #             train_batch_right_body.shape, \\\n",
    "# # #             train_batch_labels.shape)\n",
    "# #     if args.pretrained_holistic == 0 and args.expname == 'vgg16_holistic':\n",
    "# #         train_batch_loss,train_batch_predictions,train_batch_labels = trainer.train_holistic(train_batch_holistic, train_batch_labels)\n",
    "# #     elif args.pretrained_holistic == 0 and args.expname != 'vgg16_holistic':\n",
    "# #         train_batch_loss,train_batch_predictions,train_batch_labels = trainer.train_rest_multiple_loss(train_batch_holistic, \\\n",
    "# #                                                                                     train_batch_header, \\\n",
    "# #                                                                                     train_batch_footer, \\\n",
    "# #                                                                                     train_batch_left_body, \\\n",
    "# #                                                                                     train_batch_right_body, \\\n",
    "# #                                                                                     train_batch_labels)\n",
    "# #     elif args.pretrained_holistic == 1:\n",
    "# #         train_batch_loss,train_batch_predictions,train_batch_labels = trainer.train_rest(train_batch_holistic, \\\n",
    "# #                                 train_batch_header, \\\n",
    "# #                                 train_batch_footer, \\\n",
    "# #                                 train_batch_left_body, \\\n",
    "# #                                 train_batch_right_body, \\\n",
    "# #                                 train_batch_labels)\n",
    "        \n",
    "# #     elif args.pretrained_holistic == 2:\n",
    "# #         train_batch_loss,train_batch_predictions,train_batch_labels = trainer.train_ensemble(train_batch_holistic, \\\n",
    "# #                                 train_batch_header, \\\n",
    "# #                                 train_batch_footer, \\\n",
    "# #                                 train_batch_left_body, \\\n",
    "# #                                 train_batch_right_body, \\\n",
    "# #                                 train_batch_labels)\n",
    "\n",
    "\n",
    "# #     train_accuracy = metrics.accuracy(train_batch_predictions, train_batch_labels)\n",
    "#     # ----------------- Dev ---------------------\n",
    "# #     for batch in tqdm(batch_dev_data, desc='Dev batches..'):\n",
    "#     dev_batch_holistic, \\\n",
    "#             dev_batch_header, \\\n",
    "#             dev_batch_footer, \\\n",
    "#             dev_batch_left_body, \\\n",
    "#             dev_batch_right_body, \\\n",
    "#             dev_batch_labels = dev_dataset[batch_dev_data]\n",
    "\n",
    "#     if args.pretrained_holistic == 0 and args.expname == 'vgg16_holistic':\n",
    "#         dev_batch_loss, dev_batch_predictions, dev_batch_labels = trainer.test_holistic(dev_batch_holistic, dev_batch_labels)\n",
    "#     elif args.pretrained_holistic == 0 and args.expname != 'vgg16_holistic':\n",
    "#         dev_batch_loss, dev_batch_predictions, dev_batch_labels = trainer.test_rest_multiple_loss(dev_batch_holistic, \\\n",
    "#                                                                                 dev_batch_header, \\\n",
    "#                                                                                 dev_batch_footer, \\\n",
    "#                                                                                 dev_batch_left_body, \\\n",
    "#                                                                                 dev_batch_right_body, \\\n",
    "#                                                                                 dev_batch_labels)\n",
    "#     elif args.pretrained_holistic == 1:\n",
    "#         dev_batch_loss, dev_batch_predictions, dev_batch_labels = trainer.test_rest(dev_batch_holistic, \\\n",
    "#                                 dev_batch_header, \\\n",
    "#                                 dev_batch_footer, \\\n",
    "#                                 dev_batch_left_body, \\\n",
    "#                                 dev_batch_right_body, \\\n",
    "#                                 dev_batch_labels)\n",
    "\n",
    "#     elif args.pretrained_holistic == 2:\n",
    "#         dev_batch_loss, dev_batch_predictions, dev_batch_labels = trainer.train_ensemble(dev_batch_holistic, \\\n",
    "#                                 dev_batch_header, \\\n",
    "#                                 dev_batch_footer, \\\n",
    "#                                 dev_batch_left_body, \\\n",
    "#                                 dev_batch_right_body, \\\n",
    "#                                 dev_batch_labels)\n",
    "\n",
    "#     dev_accuracy = metrics.accuracy(dev_batch_predictions, dev_batch_labels)\n",
    "#     # ----------------- Test ---------------------\n",
    "# #     for batch in tqdm(batch_test_data, desc='Test batches..'):\n",
    "#     test_batch_holistic, \\\n",
    "#             test_batch_header, \\\n",
    "#             test_batch_footer, \\\n",
    "#             test_batch_left_body, \\\n",
    "#             test_batch_right_body, \\\n",
    "#             test_batch_labels = test_dataset[batch_test_data]\n",
    "\n",
    "#     if args.pretrained_holistic == 0 and args.expname == 'vgg16_holistic':\n",
    "#         test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_holistic(test_batch_holistic, test_batch_labels)\n",
    "#     elif args.pretrained_holistic == 0 and args.expname != 'vgg16_holistic':\n",
    "#         test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_rest_multiple_loss(test_batch_holistic, \\\n",
    "#                                                                                 test_batch_header, \\\n",
    "#                                                                                 test_batch_footer, \\\n",
    "#                                                                                 test_batch_left_body, \\\n",
    "#                                                                                 test_batch_right_body, \\\n",
    "#                                                                                 test_batch_labels)\n",
    "#     elif args.pretrained_holistic == 1:\n",
    "#         test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_rest(test_batch_holistic, \\\n",
    "#                                 test_batch_header, \\\n",
    "#                                 test_batch_footer, \\\n",
    "#                                 test_batch_left_body, \\\n",
    "#                                 test_batch_right_body, \\\n",
    "#                                 test_batch_labels)\n",
    "\n",
    "#     elif args.pretrained_holistic == 2:\n",
    "#             test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_ensemble(test_batch_holistic, \\\n",
    "#                                     test_batch_header, \\\n",
    "#                                     test_batch_footer, \\\n",
    "#                                     test_batch_left_body, \\\n",
    "#                                     test_batch_right_body, \\\n",
    "#                                     test_batch_labels)\n",
    "\n",
    "#     test_accuracy = metrics.accuracy(test_batch_predictions, test_batch_labels)\n",
    "    \n",
    "# #     print(np.round((epoch*100/args.epochs),2),end='\\r')\n",
    "    \n",
    "    \n",
    "#     # ----------------- Scalars ---------------------\n",
    "# #     writer.add_scalar('Loss/train',train_batch_loss.detach().cpu(), epoch)\n",
    "#     writer.add_scalar('Loss/test', test_batch_loss.detach().cpu(), epoch)\n",
    "#     writer.add_scalar('Loss/val', dev_batch_loss.detach().cpu(), epoch)\n",
    "#     writer.add_scalar('Accuracy/val', dev_accuracy, epoch)\n",
    "# #     writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "#     writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "\n",
    "#     if best > test_batch_loss:\n",
    "#         best = test_batch_loss\n",
    "#         checkpoint = {'model': trainer.model.state_dict(), 'optim': trainer.optimizer.state_dict(),\n",
    "#                       'loss': test_batch_loss, 'accuracy': test_accuracy,\n",
    "#                       'args': args, 'epoch': epoch }\n",
    "# #         logger.debug('==> New optimum found, checkpointing everything now...')\n",
    "#         torch.save(checkpoint, '%s.pt' % os.path.join(args.save, args.expname))\n",
    "#         #np.savetxt(\"test_pred.csv\", test_pred.numpy(), delimiter=\",\")\n",
    "#         early_stop_count = 0\n",
    "#     else:\n",
    "#         early_stop_count = early_stop_count + 1\n",
    "\n",
    "#         if early_stop_count == 1250*10:\n",
    "#             print('Early Stopping')\n",
    "#             print('Epoch {:7d},' \\\n",
    "#           'Dev Loss: {:.2f}, Dev Accuracy: {:.2f}, Test Loss: {:.2f},' \\\n",
    "#           'Test Accuracy: {:.2f}'.format(epoch + 1 \\\n",
    "# #                                      , train_batch_loss.detach().cpu() \\\n",
    "# #                                      , train_accuracy \\\n",
    "#                                      , dev_batch_loss.detach().cpu() \\\n",
    "#                                      , dev_accuracy \\\n",
    "#                                      , test_batch_loss.detach().cpu() \\\n",
    "#                                      , test_accuracy))\n",
    "    \n",
    "#             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15999\n"
     ]
    }
   ],
   "source": [
    "batch_test_idxs = []\n",
    "counter = 0\n",
    "\n",
    "while counter<len(test_dataset):\n",
    "    batch_test_idx = np.array(list(np.arange(counter, counter+args.batchsize)))\n",
    "    batch_test_idxs.append(batch_test_idx)\n",
    "    counter+=args.batchsize\n",
    "batch_test_idxs\n",
    "\n",
    "test_acc = []\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "# ----------------- Test ---------------------\n",
    "for batch_test_idx in batch_test_idxs:\n",
    "    print(np.round(batch_test_idx[-1]*100/16000,2), end='\\r')\n",
    "    try:\n",
    "        test_batch_holistic, \\\n",
    "                test_batch_header, \\\n",
    "                test_batch_footer, \\\n",
    "                test_batch_left_body, \\\n",
    "                test_batch_right_body, \\\n",
    "                test_batch_labels = test_dataset[batch_test_idx]\n",
    "\n",
    "        if args.pretrained_holistic == 0 and 'multiple_loss' not in args.expname: # singleloss\n",
    "            test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_holistic(test_batch_header, test_batch_labels)\n",
    "        elif args.pretrained_holistic == 0 and 'multiple_loss' in args.expname: # multipleloss\n",
    "            test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_rest_multiple_loss(test_batch_holistic, \\\n",
    "                                                                                    test_batch_header, \\\n",
    "                                                                                    test_batch_footer, \\\n",
    "                                                                                    test_batch_left_body, \\\n",
    "                                                                                    test_batch_right_body, \\\n",
    "                                                                                    test_batch_labels)\n",
    "        elif args.pretrained_holistic == 1:\n",
    "            test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_rest(test_batch_holistic, \\\n",
    "                                    test_batch_header, \\\n",
    "                                    test_batch_footer, \\\n",
    "                                    test_batch_left_body, \\\n",
    "                                    test_batch_right_body, \\\n",
    "                                    test_batch_labels)\n",
    "            \n",
    "        elif args.pretrained_holistic == 2:\n",
    "#             print('here')\n",
    "            test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test_ensemble(test_batch_holistic, \\\n",
    "                                    test_batch_header, \\\n",
    "                                    test_batch_footer, \\\n",
    "                                    test_batch_left_body, \\\n",
    "                                    test_batch_right_body, \\\n",
    "                                    test_batch_labels)\n",
    "    #         test_predictions.append(test_batch_predictions)\n",
    "    #         test_labels.append(test_batch_labels)\n",
    "    #         test_loss = test_loss + test_batch_loss\n",
    "\n",
    "        test_accuracy = metrics.accuracy(test_batch_predictions, test_batch_labels)\n",
    "    #     print(test_accuracy)\n",
    "        test_acc.append(test_accuracy)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy of vgg16_ensemble_224: 73.55 %'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Accuracy of {0}: {1} %'.format(args.expname, np.round(np.mean(test_acc)*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os,numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = np.array(sorted(os.listdir('data/test/holistic/')))\n",
    "# _,cts = np.unique(l,return_counts=True)\n",
    "# cts[cts>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('data/labels/test.csv',sep=' ')\n",
    "# np.unique(l,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# # Image.open('data/test/holistic/15999.jpg', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir('data/test/holistic/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labels = pd.read_csv('data/stratified/rvl-cdip/labels/train.csv',names=['fn','l'],sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = df_labels[df_labels['l']==12]['fn'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.random.randint(0,5000,64)\n",
    "# images = []\n",
    "# for f in filenames[idx]:\n",
    "#     images.append(np.array(Image.open(os.path.join('data/stratified/rvl-cdip/images/',f))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt-gpu)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

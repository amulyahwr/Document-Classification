{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import random\n",
    "import logging\n",
    "import config\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# NEURAL NETWORK MODULES/LAYERS\n",
    "from model import *\n",
    "from dataset import Dataset\n",
    "# METRICS CLASS FOR EVALUATION\n",
    "from metrics import Metrics\n",
    "# CONFIG PARSER\n",
    "from config import parse_args\n",
    "# TRAIN AND TEST HELPER FUNCTIONS\n",
    "from trainer import Trainer\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# MAIN BLOCK\n",
    "# def main():\n",
    "# global args\n",
    "# args = parse_args()\n",
    "# global logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.data='./data'\n",
    "        self.save='checkpoints/'\n",
    "        self.expname='test'\n",
    "        self.expno=0\n",
    "        self.input_dim=300\n",
    "        self.pretrained_model='vgg16'\n",
    "        self.num_classes=16\n",
    "        # training arguments\n",
    "        self.epochs=1000\n",
    "        self.batchsize=32\n",
    "        self.lr=0.001\n",
    "        self.wd=1e-4\n",
    "        self.sparse=False\n",
    "        self.optim='adam'\n",
    "        # miscellaneous options\n",
    "        self.seed=123\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.cuda else \"cpu\")\n",
    "#         cuda_parser = parser.add_mutually_exclusive_group(required=False)\n",
    "#         cuda', dest='cuda', action='store_true')\n",
    "#         cuda_parser.add_argument('--no-cuda', dest='cuda', action='store_false')\n",
    "#         parser.set_defaults(cuda=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = config()\n",
    "args.batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-12 16:22:06,974] DEBUG:dc_log:<__main__.config object at 0x7f72185644a8>\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('dc_log')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\")\n",
    "# file logger\n",
    "fh = logging.FileHandler(os.path.join(args.save, args.expname)+'.log', mode='w')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "# console logger\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "# argument validation\n",
    "if args.sparse and args.wd!=0:\n",
    "    logger.error('Sparsity and weight decay are incompatible, pick one!')\n",
    "    exit()\n",
    "logger.debug(args)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "\n",
    "\n",
    "\n",
    "# initialize model, criterion/loss_function, optimizer\n",
    "if args.pretrained_model == 'vgg16':\n",
    "    pretrained_model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze training for all layers\n",
    "for param in pretrained_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model = DocClassification(args, pretrained_model)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda(), criterion.cuda()\n",
    "\n",
    "if args.optim=='adam':\n",
    "    optimizer   = optim.Adam(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "elif args.optim=='adagrad':\n",
    "    optimizer   = optim.Adagrad(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "elif args.optim=='sgd':\n",
    "    optimizer   = optim.SGD(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "elif args.optim == 'adadelta':\n",
    "    optimizer = optim.Adadelta(parameters, lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "metrics = Metrics(args.num_classes)\n",
    "\n",
    "# create trainer object for training and testing\n",
    "trainer = Trainer(args, model, criterion, optimizer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-12 16:22:13,983] DEBUG:dc_log:==> Size of train data   : 1000 \n",
      "[2019-10-12 16:22:13,985] DEBUG:dc_log:==> Size of val data   : 100 \n",
      "[2019-10-12 16:22:13,987] DEBUG:dc_log:==> Size of test data   : 100 \n"
     ]
    }
   ],
   "source": [
    "train_dir = glob.glob(os.path.join(args.data,'train/holistic/*.pt'))\n",
    "dev_dir = glob.glob(os.path.join(args.data,'val/holistic/*.pt'))\n",
    "test_dir = glob.glob(os.path.join(args.data,'test/holistic/*.pt'))\n",
    "\n",
    "#print(train_dir, dev_dir, test_dir)\n",
    "\n",
    "train_dataset = Dataset(os.path.join(args.data,'train'), train_dir)\n",
    "dev_dataset = Dataset(os.path.join(args.data,'val'), dev_dir)\n",
    "test_dataset = Dataset(os.path.join(args.data,'test'), test_dir)\n",
    "\n",
    "\n",
    "logger.debug('==> Size of train data   : %d ' % len(train_dataset))\n",
    "logger.debug('==> Size of val data   : %d ' % len(dev_dataset))\n",
    "logger.debug('==> Size of test data   : %d ' % len(test_dataset))\n",
    "\n",
    "train_idx = list(np.arange(len(train_dataset)))\n",
    "dev_idx = list(np.arange(len(dev_dataset)))\n",
    "test_idx = list(np.arange(len(test_dataset)))\n",
    "\n",
    "best = float('inf')\n",
    "columns = ['ExpName','ExpNo', 'Epoch', 'Loss','Accuracy']\n",
    "results = []\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batches..:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches..:   0%|          | 0/32 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-26c013b99da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training batches..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbatch_holistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_holistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training batches..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research4/projects/document_classification/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch, labels)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1822\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    dev_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    dev_predictions = []\n",
    "    dev_labels = []\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "    random.shuffle(dev_idx)\n",
    "    random.shuffle(test_idx)\n",
    "\n",
    "    batch_train_data = [train_idx[i:i + args.batchsize] for i in range(0, len(train_idx), args.batchsize)]\n",
    "    batch_dev_data = [dev_idx[i:i + args.batchsize] for i in range(0, len(dev_idx), args.batchsize)]\n",
    "    batch_test_data = [test_idx[i:i + args.batchsize] for i in range(0, len(test_idx), args.batchsize)]\n",
    "\n",
    "    for batch in tqdm(batch_train_data, desc='Training batches..'):\n",
    "        batch_holistic, _, _, _, _, batch_labels = train_dataset[batch]\n",
    "        _ = trainer.train(batch_holistic, batch_labels)\n",
    "\n",
    "    for batch in tqdm(batch_train_data, desc='Training batches..'):\n",
    "        train_batch_holistic, _, _, _, _, train_batch_labels = train_dataset[batch]\n",
    "        train_batch_loss, train_batch_predictions, train_batch_labels = trainer.test(train_batch_holistic, train_batch_labels)\n",
    "\n",
    "        train_predictions.append(train_batch_predictions)\n",
    "        train_labels.append(train_batch_labels)\n",
    "        train_loss = train_loss + train_batch_loss\n",
    "\n",
    "    train_accuracy = metrics.accuracy(np.concatenate(train_predictions), np.concatenate(train_labels))\n",
    "\n",
    "    for batch in tqdm(batch_dev_data, desc='Dev batches..'):\n",
    "        dev_batch_holistic, _, _, _, _, dev_batch_labels = dev_dataset[batch]\n",
    "        dev_batch_loss, dev_batch_predictions, dev_batch_labels = trainer.test(dev_batch_holistic, dev_batch_labels)\n",
    "\n",
    "        dev_predictions.append(dev_batch_predictions)\n",
    "        dev_labels.append(dev_batch_labels)\n",
    "        dev_loss = dev_loss + dev_batch_loss\n",
    "\n",
    "    dev_accuracy = metrics.accuracy(np.concatenate(dev_predictions), np.concatenate(dev_labels))\n",
    "\n",
    "    for batch in tqdm(batch_test_data, desc='Test batches..'):\n",
    "        test_batch_holistic, _, _, _, _, test_batch_labels = test_dataset[batch]\n",
    "        test_batch_loss, test_batch_predictions, test_batch_labels = trainer.test(test_batch_holistic, test_batch_labels)\n",
    "\n",
    "        test_predictions.append(test_batch_predictions)\n",
    "        test_labels.append(test_batch_labels)\n",
    "        test_loss = test_loss + test_batch_loss\n",
    "\n",
    "    test_accuracy = metrics.accuracy(np.concatenate(test_predictions), np.concatenate(test_labels))\n",
    "\n",
    "    logger.info('==> Training Epoch {}, \\\n",
    "                    \\nLoss: {}, \\\n",
    "                    \\nAccuracy: {}'.format(epoch + 1, \\\n",
    "                                        train_loss/(len(batch_train_data) * args.batchsize), \\\n",
    "                                        train_accuracy))\n",
    "    logger.info('==> Dev Epoch {}, \\\n",
    "                    \\nLoss: {}, \\\n",
    "                    \\nAccuracy: {}'.format(epoch + 1, \\\n",
    "                                        dev_loss/(len(batch_dev_data) * args.batchsize), \\\n",
    "                                        dev_accuracy))\n",
    "\n",
    "    logger.info('==> Test Epoch {}, \\\n",
    "                    \\nLoss: {}, \\\n",
    "                    \\nAccuracy: {}'.format(epoch + 1, \\\n",
    "                                        test_loss/(len(batch_test_data) * args.batchsize), \\\n",
    "                                        test_accuracy))\n",
    "    #quit()\n",
    "    results.append((args.expname, \\\n",
    "                    args.expno, \\\n",
    "                    epoch+1, \\\n",
    "                    test_loss/(len(batch_test_data) * args.batchsize), \\\n",
    "                    test_accuracy))\n",
    "\n",
    "    if best > test_loss:\n",
    "        best = test_loss\n",
    "        checkpoint = {'model': trainer.model.state_dict(), 'optim': trainer.optimizer,\n",
    "                      'loss': test_loss, 'accuracy': test_accuracy,\n",
    "                      'args': args, 'epoch': epoch }\n",
    "        logger.debug('==> New optimum found, checkpointing everything now...')\n",
    "        torch.save(checkpoint, '%s.pt' % os.path.join(args.save, args.expname))\n",
    "        #np.savetxt(\"test_pred.csv\", test_pred.numpy(), delimiter=\",\")\n",
    "    else:\n",
    "        early_stop_count = early_stop_count + 1\n",
    "\n",
    "        if early_stop_count == 20:\n",
    "            quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((1, 3, 225, 225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt-gpu)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
